# ğŸ§  Apache JIRA â†’ JSONL Scraper (Technical Assignment Submission)
### Candidate: **Prudhvi Raj**
### Repository: [https://github.com/prudhvilabcd/apache-jira-scraper](https://github.com/prudhvilabcd/apache-jira-scraper)

---

## ğŸ“˜ Overview  
This project scrapes public issues from **[Apacheâ€™s JIRA portal](https://issues.apache.org/jira)** for selected open-source projects such as Hadoop, Hive, and HBase.  
It automatically handles pagination, rate limits, and transforms raw HTML/JSON data into clean **JSONL** format suitable for ML or analytics.

âœ… **Goal:** Build a reliable, resumable web scraping pipeline that produces structured datasets from Apache JIRA.  

---

## âš™ï¸ Features Implemented

| Feature | Description | Status |
|----------|--------------|--------|
| **Data Scraping** | Fetches issues, metadata, and comments | âœ… |
| **Pagination Handling** | Iteratively scrapes multiple pages efficiently | âœ… |
| **Error Handling** | Handles HTTP 429 & 504 with exponential backoff | âœ… |
| **Resume Capability** | Uses checkpoints to continue from the last successful state | âœ… |
| **Data Transformation** | Converts HTML â†’ Plain Text using BeautifulSoup | âœ… |
| **JSONL Output** | Outputs one issue per line in structured JSONL format | âœ… |
| **Configurable Parameters** | Project keys, retry count, page size in YAML config | âœ… |
| **Incremental Checkpoints** | Updates after each issue for fault tolerance | âœ… |

---

## ğŸ§© System Architecture

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py          # Entry point â€“ orchestrates scraping
â”‚   â”œâ”€â”€ scraper.py       # Fetches data from JIRA REST API
â”‚   â”œâ”€â”€ transform.py     # Cleans & converts data to JSONL
â”‚   â”œâ”€â”€ utils.py         # Checkpoint manager, helpers
â”‚
â”œâ”€â”€ checkpoints/         # Stores progress for each project
â”œâ”€â”€ data/                # Final JSONL output files
â”œâ”€â”€ config.yaml          # Configuration file (editable)
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md            # Documentation (this file)
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create Virtual Environment
```bash
python -m venv venv
venv\Scripts\activate  # For Windows
# source venv/bin/activate  # For Linux/Mac
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## âš™ï¸ Configuration (config.yaml)

```yaml
projects:
  - HADOOP
  - HBASE
  - HIVE

output_dir: data
page_size: 100

retry:
  max_retries: 5
  base_sleep_seconds: 2
  max_sleep_seconds: 60
```

ğŸ”¹ You can replace projects with any Apache JIRA key (e.g., `KAFKA`, `SPARK`, `ZOOKEEPER`).  

---

## â–¶ï¸ Run the Project

```bash
python -m src.main --config config.yaml
```

This will generate files under `/data/{PROJECT}.json` and update `/checkpoints/{PROJECT}.json`.  

---

## ğŸ“Š Example Output (JSONL)

```json
{
  "project": "HADOOP",
  "issue_key": "HADOOP-9999",
  "metadata": {
    "title": "Fix NullPointerException in namenode",
    "status": "Open",
    "priority": "Major",
    "assignee": "user123",
    "created": "2025-11-07T11:23:45",
    "updated": "2025-11-07T12:34:56"
  },
  "description": "Detailed issue description here...",
  "comments": ["plain text comment 1", "plain text comment 2"],
  "classification_target": "Open"
}
```

---

## ğŸ§  Design & Architecture Highlights

| Area | Implementation |
|------|----------------|
| **API-based Scraping** | Uses JIRA REST API (`/rest/api/2/search`) instead of HTML for efficiency |
| **Resumable Execution** | Checkpoints for each project (`last_updated_iso`, `seen_keys`) |
| **Transformation Layer** | Cleans HTML â†’ plain text and appends derived fields |
| **Fault Tolerance** | Retries failed requests with exponential backoff |
| **Extensibility** | Modular codebase; can add new projects easily |

---

## âš¡ Performance Metrics

| Project | Issues Scraped | Time Taken | Speed |
|----------|----------------|-------------|--------|
| **HADOOP** | 13,615 | 26s | ~5.4 issues/s |
| **HBASE** | 29,468 | 2h 3m | ~3.9 issues/s |
| **HIVE** | 29,978 | 1h 44m | ~4.7 issues/s |

---

## ğŸš€ Future Enhancements
- Multi-threaded scraping for faster speed  
- Integrate summarization or NLP preprocessing for JSONL datasets  
- Add Dockerfile + CI/CD setup  
- SQLite backend for stronger checkpointing  

---

## ğŸ§¾ Notes for Evaluation
- All data collected from **public Apache JIRA endpoints** â€” no authentication required  
- Dataset stored as `.jsonl` for downstream ML/LLM use cases  
- Code is **idempotent** (safe to re-run) and **recoverable** from last checkpoint  

---

## ğŸ“‚ Data Access (Google Drive)
Due to GitHubâ€™s 25MB file upload limit, the full dataset (â‰ˆ900MB JSONL) is stored here:  
ğŸ“ **[Google Drive Link â€“ Data Folder](https://drive.google.com/drive/folders/1hkzbmajJ4ofY4zS58_hg-Z90mP3xjit?usp=sharing)**  

---

## ğŸ“¥ Repository Access
ğŸ”— **GitHub Repo:** [https://github.com/prudhvilabcd/apache-jira-scraper](https://github.com/prudhvilabcd/apache-jira-scraper)  
ğŸ“¦ **ZIP Download:** [Click to Download ZIP](https://github.com/prudhvilabcd/apache-jira-scraper/archive/refs/heads/main.zip)

---

## ğŸ‘¨â€ğŸ’» Author
**Name:** Prudhvi Raj  
**Role:** M.Tech (CSE) Student, NIT Delhi  
**Focus Areas:** AI, ML, and Data Mining  

---

## â­ Support
If this project was part of your review, please â­ it on GitHub â€” it helps recognition and visibility.

---
